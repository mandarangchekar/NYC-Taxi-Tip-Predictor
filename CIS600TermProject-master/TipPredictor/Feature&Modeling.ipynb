{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###IMPORTING LIBRARIES"
      ],
      "metadata": {
        "id": "xmAO8nAcNlUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import sklearn\n",
        "from sklearn import datasets, linear_model, metrics, svm, cluster\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "outputs": [],
      "metadata": {
        "id": "u5bHIlYiNlUC"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "id": "5EKZAeHVNlUE"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###READING DATA"
      ],
      "metadata": {
        "id": "OfW1aciMNlUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "master_df = pd.read_csv(\"../data/final.csv\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-37be119d9ead>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaster_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/final.csv'"
          ]
        }
      ],
      "metadata": {
        "id": "u5lKs88wNlUE",
        "outputId": "6c401474-d8e7-4639-d6d6-4491841b9481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using df - a dataframe with smaller subset of the data"
      ],
      "metadata": {
        "id": "ZxKyHkiyNlUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = master_df[:100000]\n",
        "# df[' fare_amount']"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZAvUelVSNlUF"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATA CLEANING"
      ],
      "metadata": {
        "id": "HtuOsVooNlUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply data preprocessing\n",
        "for i in range(df.shape[0]):\n",
        "  # Check if the fare_amount for the current row is 0\n",
        "    if df[' trip_distance'][i] > 1000.0:\n",
        "      # If fare_amount is 0, set it to NaN using numpy\n",
        "        df[' trip_distance'][i] = np.nan"
      ],
      "outputs": [],
      "metadata": {
        "id": "vHcnCAqKNlUF"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each row in the dataframe\n",
        "for i in range(df.shape[0]):\n",
        "  # Check if the fare_amount value in the current row is 0\n",
        "    if df[' fare_amount'][i] == 0:\n",
        "      # Replace the fare_amount value in the current row with NaN\n",
        "        df[' fare_amount'][i] = np.nan"
      ],
      "outputs": [],
      "metadata": {
        "id": "EkQIW8xLNlUF"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()\n",
        "df.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "GUPs8jW7NlUF"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for passenger counts greater than 6 or equal to 0 and replace with NaN\n",
        "for i in range(df.shape[0]):\n",
        "    if df[' passenger_count'][i] > 6 or df[' passenger_count'][i] == 0:\n",
        "        df[' passenger_count'][i] = np.nan"
      ],
      "outputs": [],
      "metadata": {
        "id": "_IBRJhFYNlUF"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for unknown payment types and replace with NaN\n",
        "for i in range(df.shape[0]):\n",
        "    if df[' payment_type'][i] == 'UNK':\n",
        "        df[' payment_type'][i] = np.nan"
      ],
      "outputs": [],
      "metadata": {
        "id": "5JDzUNUuNlUG"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for tip amounts that are strings and replace with NaN\n",
        "for i in range(df.shape[0]):\n",
        "    if type(df[' tip_amount'][i]) == str:\n",
        "        df[' tip_amount'][i] = np.nan"
      ],
      "outputs": [],
      "metadata": {
        "id": "1DZMa3ZmNlUG"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####K-Means clustering for calculating tip bins"
      ],
      "metadata": {
        "id": "vrzrOeA_NlUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KMeans clustering model with 5 clusters\n",
        "km = sklearn.cluster.KMeans(n_clusters = 5)\n",
        "# Fit the KMeans model to the tip_amount values in the dataframe\n",
        "km.fit(df[[' tip_amount']].values)"
      ],
      "outputs": [],
      "metadata": {
        "id": "k9x463P6NlUG"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign each data point to a cluster based on the KMeans model\n",
        "df['cluster_class']= km.labels_"
      ],
      "outputs": [],
      "metadata": {
        "id": "UFh8vQKcNlUG"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "km.cluster_centers_"
      ],
      "outputs": [],
      "metadata": {
        "id": "R6U3ywydNlUG"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UTILS FOR FEATURES\n",
        "\n",
        "####Date Time Features"
      ],
      "metadata": {
        "id": "dyuJH3J2NlUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the datetime module to manipulate dates and times\n",
        "from datetime import datetime\n",
        "# Define a function to interpret the pickup time in a dataframe\n",
        "def InterpretPickupTime(df, field):\n",
        "    count = df.count()[0]\n",
        "    month = [0]*count #1-12\n",
        "    hourOfDay = [0]*count #0-23\n",
        "    dayOfWeek = [0]*count #0=monday, 6=sunday\n",
        "    weekend = [\"X\"]*count #weekday/weekend\n",
        "    timegroup = [\"X\"]*count #0-5early, 6-10morning, 11-15afternoon, 16-20evening, 21-23night\n",
        "    # Try to convert the pickup time in each row of the dataframe to a datetime object, and extract month, hour of day, day of week, weekend, and time group values\n",
        "    try:\n",
        "        for i in range(count):\n",
        "            dt = datetime.strptime(df[field][i],\"%Y-%m-%d %H:%M:%S\")\n",
        "            month[i] = dt.month\n",
        "            dayOfWeek[i] = dt.weekday()\n",
        "            hourOfDay[i] = dt.hour\n",
        "            if dt.weekday()> 4:\n",
        "                weekend[i] = \"weekend\";\n",
        "            else:\n",
        "                weekend[i] = \"weekday\";\n",
        "            if hourOfDay[i] < 6:\n",
        "                timegroup[i] = \"early\"\n",
        "            elif hourOfDay[i] < 11:\n",
        "                timegroup[i] = \"morning\"\n",
        "            elif hourOfDay[i] < 16:\n",
        "                timegroup[i] = \"afternoon\"\n",
        "            elif hourOfDay[i] < 21:\n",
        "                timegroup[i] = \"evening\"\n",
        "            else:\n",
        "                timegroup[i] = \"night\"\n",
        "    # If an exception is raised (e.g. if the pickup time is not in the expected format), print an error message and return\n",
        "    except:\n",
        "        print \"Exception##\" + str(i) + \"  \"+ df[field][i]+ \"## \"\n",
        "        return\n",
        "    # Add new columns to the dataframe to store the extracted month, hour of day, day of week, weekend, and time group values    \n",
        "    df[field+\"month\"] = month\n",
        "    df[field+\"hourOfDay\"] = hourOfDay\n",
        "    df[field+\"dayOfWeek\"] = dayOfWeek\n",
        "    df[field+\"weekend\"] = weekend\n",
        "    df[field+\"timegroup\"] = timegroup"
      ],
      "outputs": [],
      "metadata": {
        "id": "CuvbqHcfNlUH"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tip Class Features"
      ],
      "metadata": {
        "id": "nHezzP6aNlUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to classify tip amounts into different classes\n",
        "def GetTipClasses(tip):\n",
        "    if tip == 0.0:\n",
        "        return \"Z\"\n",
        "    elif tip > 0.0 and tip < 1.0:\n",
        "        return \"A\"\n",
        "    elif tip >= 1.0 and tip < 2.0:\n",
        "        return \"B\"\n",
        "    elif tip >= 2.0 and tip < 3.0:\n",
        "        return \"C\"\n",
        "    elif tip >= 3.0 and tip < 4.0:\n",
        "        return \"D\"\n",
        "    elif tip >= 4.0 and tip < 5.0:\n",
        "        return \"E\"\n",
        "    elif tip >= 5.0 and tip < 7.5:\n",
        "        return \"F\"\n",
        "    elif tip >= 7.5 and tip < 10.0:\n",
        "        return \"G\"\n",
        "    else:\n",
        "        return \"H\"\n",
        "# Define a function to calculate and assign tip classes to a dataframe\n",
        "def CalculateTipClasses(df):\n",
        "    count = len(df)\n",
        "#     print count\n",
        "    tip_df = list()\n",
        "    tip_amnt = df[\" tip_amount\"].tolist()\n",
        "    print len(tip_amnt)\n",
        "    for i in range (count):\n",
        "        tip_df.append(GetTipClasses(tip_amnt[i]))\n",
        "#     print len(tip_df)\n",
        "    df[\"tip_class\"] = np.array(tip_df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ua6o_ZwiNlUH"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TIP Class Non zero "
      ],
      "metadata": {
        "id": "3-uTiR8VNlUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to assign a letter-based tip class to a numerical tip amount\n",
        "# based on a set of predetermined ranges.\n",
        "def GetTipClasses_nz(tip):\n",
        "    if tip > 0.0 and tip < 1.0:\n",
        "        return \"A\"\n",
        "    elif tip >= 1.0 and tip < 2.0:\n",
        "        return \"B\"\n",
        "    elif tip >= 2.0 and tip < 3.0:\n",
        "        return \"C\"\n",
        "    elif tip >= 3.0 and tip < 4.0:\n",
        "        return \"D\"\n",
        "    elif tip >= 4.0 and tip < 5.0:\n",
        "        return \"E\"\n",
        "    elif tip >= 5.0 and tip < 7.5:\n",
        "        return \"F\"\n",
        "    elif tip >= 7.5 and tip < 10.0:\n",
        "        return \"G\"\n",
        "    else:\n",
        "        return \"H\"\n",
        "# Define a function to add a column to a DataFrame containing the letter-based tip classes\n",
        "# for the tip amounts in the 'tip_amount' column of the DataFrame.    \n",
        "def CalculateTipClasses_nz(df):\n",
        "    count = df.count()[0]\n",
        "    tip_df = ['X']*(count)\n",
        "    tip_amnt = df[\" tip_amount\"].tolist()\n",
        "    # Loop over each row of the DataFrame, calculate the tip class for the corresponding tip amount,\n",
        "    # and add it to the 'tip_df' list.\n",
        "    for i in range (count):\n",
        "        tip_df[i] = GetTipClasses_nz(tip_amnt[i])\n",
        "    # Add the 'tip_class_nz' column to the DataFrame containing the calculated tip classes.\n",
        "    df[\"tip_class_nz\"] = np.array(tip_df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SZE8ZZAyNlUI"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tip no Tip Features"
      ],
      "metadata": {
        "id": "nWneNnDsNlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to determine if  tip was given or not\n",
        "def GetTipNoTip(tip):\n",
        "    if tip == 0.0:\n",
        "        return \"NOTIP\"\n",
        "    else:\n",
        "        return \"TIP\"\n",
        "#function to determine if tip was given or not for all the records in the dataframe\n",
        "def CalculateTipNoTip(df):\n",
        "    count = df.count()[0]\n",
        "#     print count\n",
        "    tip_df = ['X']*count\n",
        "    tip_amnt = df[\" tip_amount\"].tolist()\n",
        "    for i in range (count):\n",
        "        tip_df[i] = GetTipNoTip(tip_amnt[i])\n",
        "    df[\"tip_no_tip\"] = np.array(tip_df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "W5F6ke2UNlUI"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TIP PERCENT"
      ],
      "metadata": {
        "id": "u_jKX1CZNlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function calculates the tip percentage given the tip amount and the fare amount.\n",
        "def GetTipPercent(tip, fare):\n",
        "        return tip*100/fare\n",
        "# This function calculates the tip percentage for each row in the dataframe.\n",
        "def CalcuateTipPercentage(df):\n",
        "    count = df.count()[0]\n",
        "    tip_df = ['X']*count\n",
        "    tip_amnt = df[\" tip_amount\"].tolist()\n",
        "    fare_amnt = df[' fare_amount'].tolist()\n",
        "    # Loop through each row of the dataframe and calculate the tip percentage.\n",
        "    for i in range (count):\n",
        "        if tip_amnt[i] == 0 or fare_amnt[i] == 0:\n",
        "            tip_df[i] = 0\n",
        "        else:\n",
        "            tip_df[i] = GetTipPercent(tip_amnt[i], fare_amnt[i])\n",
        "    df[\"tip_percent\"] = np.array(tip_df)   "
      ],
      "outputs": [],
      "metadata": {
        "id": "97DRfQsaNlUI"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Zip Code Features"
      ],
      "metadata": {
        "id": "ahF5eTAlNlUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pick the zip code with the minimum latitude that matches a given latitude and longitude\n",
        "def PickZipCode(Lat,Long, zipcodes_NY):\n",
        "    #first pick min latitude    \n",
        "    minLong = min(zipcodes_NY.longitude.tolist(), key=lambda x:abs(x-Long))\n",
        "    minLat = min(zipcodes_NY[zipcodes_NY.longitude == minLong].latitude.tolist(), key=lambda x:abs(x-Lat))\n",
        "    z = zipcodes_NY[((zipcodes_NY.longitude==minLong)&(zipcodes_NY.latitude==minLat))][\"zip\"].tolist()[0]\n",
        "    return z\n",
        "\n",
        "# Function to assign zip codes to a given field in a data frame\n",
        "def AssignZipcodes(df,field):\n",
        "    count = df.count()[0]\n",
        "    zipz = [0]*count\n",
        "    print count\n",
        "    nyc_df = pd.read_csv(\"..data/nycitydata.csv\")\n",
        "    A = nyc_df.zipcodes.tolist()\n",
        "    zipcodes_NY = pd.read_csv(\"..data/zipcode_NYC.csv\")\n",
        "    zipcodes_NY = zipcodes_NY[zipcodes_NY.state==\"NY\"]\n",
        "    zipcodes_NY.count()\n",
        "    B = zipcodes_NY.zip.tolist()\n",
        "    keepList = [x for x in B if x in A]\n",
        "    zipcodes_NY = zipcodes_NY[zipcodes_NY.zip.isin(keepList)]\n",
        "    try:\n",
        "    # Iterate through each row of the data frame and assign a zip code to the corresponding field  \n",
        "        for i in range(count):\n",
        "            if i%10000 == 0:#test\n",
        "                print i\n",
        "            zipz[i] = PickZipCode(df[\" \"+field+\"_latitude\"][i],df[\" \"+field+\"_longitude\"][i], zipcodes_NY)\n",
        "    except:\n",
        "        print \"exception \"+str(i)\n",
        "    df[field+\"zipz\"]=zipz"
      ],
      "outputs": [],
      "metadata": {
        "id": "cjIDmERANlUI"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def GetNYCityDataByZipCode():\n",
        "    nyc_df = pd.read_csv(\"..data/nycitydata.csv\")\n",
        "    zipDiction={}\n",
        "    \n",
        "    # Loop through the rows of the data and add the data to the dictionary\n",
        "\n",
        "    for i in range(nyc_df.count()[0]):\n",
        "        zipDiction[nyc_df.zipcodes[i]] = [nyc_df.costOfLivingIndex[i],nyc_df.populationDensity[i],nyc_df.percentageMales[i],nyc_df.medianHouseholdIncome[i]]     \n",
        "    return zipDiction   "
      ],
      "outputs": [],
      "metadata": {
        "id": "Sg7-nve8NlUJ"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def AddCityData(df,field):\n",
        "    # Get the number of rows in the dataframe\n",
        "    count = df.count()[0]\n",
        "    # Create empty lists to store the values of the city data fields\n",
        "    COLI = [0]*count\n",
        "    PD = [0]*count\n",
        "    PM = [0]*count\n",
        "    MHI = [0]*count\n",
        "    # Retrieve the dictionary of city data by zip code\n",
        "    diction = GetNYCityDataByZipCode()\n",
        "    print count\n",
        "    try:\n",
        "      # Loop through each row in the dataframe\n",
        "        for i in range(count):\n",
        "          # Print progress every 10000 rows\n",
        "            if i%10000 == 0:\n",
        "                print i\n",
        "            # Retrieve the city data for the zip code in the current row\n",
        "            temp = diction[df[field+\"zipz\"][i]]\n",
        "            # Store the values in the appropriate lists\n",
        "            COLI[i] = temp[0]\n",
        "            PD[i] = temp[1]\n",
        "            PM[i] = temp[2]\n",
        "            MHI[i] = temp[3]\n",
        "    except:\n",
        "        print \"exception \"+str(i)\n",
        "    # Add the city data fields to the dataframe    \n",
        "    df[field+\"CostOfLivingIndex\"]=COLI\n",
        "    df[field+\"PopulationDensity\"]=PD\n",
        "    df[field+\"PercentageMale\"]=PM\n",
        "    df[field+\"MedianHouseHoldIncome\"]=MHI\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "nIzWdOpINlUJ"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "AddCityData(df,\"dropoff\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "m-a1MzKkNlUJ"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CREATE NEW FEATURES\n",
        "\n",
        "####Time Based"
      ],
      "metadata": {
        "id": "8z1U9W9UNlUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InterpretPickupTime(df, \" pickup_datetime\")\n",
        "InterpretPickupTime(df, \" dropoff_datetime\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "aLu7NIK0NlUJ"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tip Based"
      ],
      "metadata": {
        "id": "zTS2l4eONlUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CalculateTipClasses(df)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tnwrXkeQNlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "CalculateTipNoTip(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "s2h13PseNlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "CalcuateTipPercentage(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "d6izjs9FNlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "km = sklearn.cluster.KMeans(n_clusters = 5)\n",
        "km.fit(df[['tip_percent']].values)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fpmUfhNWNlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "km.cluster_centers_"
      ],
      "outputs": [],
      "metadata": {
        "id": "I8F5TieRNlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['cluster_percent_class']= km.labels_"
      ],
      "outputs": [],
      "metadata": {
        "id": "qPAEa7g_NlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Location Based"
      ],
      "metadata": {
        "id": "TOOT7BgcNlUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AssignZipcodes(df, 'dropoff')"
      ],
      "outputs": [],
      "metadata": {
        "id": "kaf15Qq-NlUK"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MODELS"
      ],
      "metadata": {
        "id": "gBCARa4WNlUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Utils for models"
      ],
      "metadata": {
        "id": "2N4P3VUZNlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function runs Support Vector Machines (SVM) algorithm on the training data and prints the accuracy of the model\n",
        "def run_SVM(X_train, Y_train, Y_dev):\n",
        "    svm_clf = svm.SVC(kernel='rbf')\n",
        "    svm_clf.fit(X_train, Y_train.reshape(Y_train.shape[0],))\n",
        "    Y_pred_dev = svm_clf.predict(X_dev)\n",
        "    SVM_acc = float(sum(map(lambda x,y : x==y, Y_pred_dev, Y_dev)))/len(Y_dev)\n",
        "    print \"SVM RBF Kernel set accuracy: %f\" %((SVM_acc)*100)\n",
        "\n",
        "# This function runs decision tree algorithm on the training data and prints the accuracy of the model\n",
        "def run_decision_tree(X_train, Y_train, Y_dev):\n",
        "    dtc_clf = tree.DecisionTreeClassifier()\n",
        "    dtc_clf.fit(X_train, Y_train)\n",
        "    Y_pred_dev1 = dtc_clf.predict(X_dev)\n",
        "    DTC_acc = float(sum(map(lambda x,y : x==y, Y_pred_dev1, Y_dev)))/len(Y_dev)\n",
        "    print \"DRC set accuracy: %f\" %((DTC_acc)*100)\n",
        "\n",
        "# This function runs random forests algorithm on the training data and prints the accuracy of the model\n",
        "def run_random_forests(X_train, Y_train, Y_dev):\n",
        "    rf_clf = RandomForestClassifier(n_estimators=10)\n",
        "    rf_clf.fit(X_train, Y_train)\n",
        "    Y_pred_dev1 = rf_clf.predict(X_dev)\n",
        "    DTC_acc = float(sum(map(lambda x,y : x==y, Y_pred_dev1, Y_dev)))/len(Y_dev)\n",
        "    print \"RF set accuracy: %f\" %((DTC_acc)*100)\n",
        "    \n",
        "# This function runs AdaBoost algorithm on the training data and prints the accuracy of the model    \n",
        "def run_Adaboost(X_train, Y_train, Y_dev):\n",
        "    adb_clf = AdaBoostClassifier(base_estimator=None, n_estimators=50)\n",
        "    adb_clf.fit(X_train, Y_train)\n",
        "    Y_pred_dev1 = adb_clf.predict(X_dev)\n",
        "    DTC_acc = float(sum(map(lambda x,y : x==y, Y_pred_dev1, Y_dev)))/len(Y_dev)\n",
        "    print \"Adaboost set accuracy: %f\" %((DTC_acc)*100)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hvjsxxE8NlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print [(col, pd.isnull(df[col]).sum()) for col in df.columns]"
      ],
      "outputs": [],
      "metadata": {
        "id": "zCEl1d_5NlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def GetTrainingAndTest(df):\n",
        "    tr = int(math.floor(0.7*df.count()[0]))\n",
        "    training = df[0:tr]\n",
        "    test = df[tr:]\n",
        "    return training,test"
      ],
      "outputs": [],
      "metadata": {
        "id": "W79DUQZMNlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [ ' fare_amount', ' payment_type', ' tolls_amount', 'pickupzipz','pickupCostOfLivingIndex']\n",
        "\n",
        "#'dropoffzipz','dropoffCostOfLivingIndex',  trip_distance', ' trip_time_in_secs',\n",
        "#' passenger_count', ' pickup_datetimeweekend', ' dropoff_datetimeweekend'"
      ],
      "outputs": [],
      "metadata": {
        "id": "U1-rm0mgNlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tip No-Tip Classification Model"
      ],
      "metadata": {
        "id": "97S02lNKNlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataset into train and test sets\n",
        "train_df,test_df = GetTrainingAndTest(df)\n",
        "dvec = DictVectorizer()\n",
        "#Converting feature columns of training set into a dictionary of key-value pairs and transforming it into an array\n",
        "dict_df = train_df[feature_cols].T.to_dict().values()\n",
        "train_data = pd.DataFrame(dvec.fit_transform(dict_df).toarray())\n",
        "#Converting feature columns of test set into a dictionary of key-value pairs and transforming it into an array\n",
        "dict_df = test_df[feature_cols].T.to_dict().values()\n",
        "test_data = pd.DataFrame(dvec.transform(dict_df).toarray())\n",
        "#X_train and X_dev, and the corresponding target variables to Y_train and Y_dev\n",
        "X_train = train_data\n",
        "Y_train =  train_df['tip_no_tip'].reshape(len(train_df['tip_no_tip']), 1)\n",
        "\n",
        "X_dev = test_data\n",
        "Y_dev = test_df['tip_no_tip'].reshape(len(test_df['tip_no_tip']), 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gapR4L15NlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "heaPieRVNlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_SVM(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xxrAbBU5NlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decision Tree Classifier"
      ],
      "metadata": {
        "id": "1LIY-FwlNlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_decision_tree(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jiiwlr2YNlUL"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Classifier"
      ],
      "metadata": {
        "id": "Kb0iJbvSNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_random_forests(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wG13QI3eNlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Non zero Tip Classification Model"
      ],
      "metadata": {
        "id": "ucVRqvbbNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code calculates tip classes for all trips in the given dataframe.\n",
        "CalculateTipClasses(master_df)\n",
        "#This code interprets and extracts the pickup time from the given column in the dataframe.\n",
        "InterpretPickupTime(master_df, \" pickup_datetime\")\n",
        "#This code interprets and extracts the dropoff time from the given column in the dataframe.\n",
        "InterpretPickupTime(master_df, \" dropoff_datetime\")\n",
        "#This code filters out the rows where tip amount is greater than zero from the given dataframe and takes first 100000 rows.\n",
        "df_tipnz= master_df[master_df[' tip_amount'] > 0.0][:100000]\n",
        "#Then it prints the shape and unique values of the tip_class column for this filtered dataframe.\n",
        "print df_tipnz.shape\n",
        "df.shape\n",
        "print df_tipnz.tip_class.unique()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jIJ1OKI7NlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data into training and test sets\n",
        "train_df,test_df = GetTrainingAndTest(df_tipnz)\n",
        "\n",
        "dvec = DictVectorizer()\n",
        "#Extracting the features from training set and converting them into a dictionary\n",
        "dict_df = train_df[feature_cols].T.to_dict().values()\n",
        "#Converting the dictionary of features into an array and transforming it using DictVectorizer\n",
        "train_data = pd.DataFrame(dvec.fit_transform(dict_df).toarray())\n",
        "#Extracting the features from test set and converting them into a dictionary\n",
        "\n",
        "dict_df = test_df[feature_cols].T.to_dict().values()\n",
        "#Converting the dictionary of features into an array and transforming it using DictVectorizer\n",
        "test_data = pd.DataFrame(dvec.transform(dict_df).toarray())\n",
        "#Assigning training features and labels to variables\n",
        "X_train = train_data\n",
        "Y_train =  train_df['tip_class'].reshape(len(train_df['tip_class']), 1)\n",
        "#Assigning test features and labels to variables\n",
        "X_dev = test_data\n",
        "Y_dev = test_df['tip_class'].reshape(len(test_df['tip_class']), 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HGrfBJq6NlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "xKiF2HNdNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_SVM(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SkQ5WFDJNlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decision Tree Classifier"
      ],
      "metadata": {
        "id": "wZB89W3HNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_decision_tree(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Uq0TlANoNlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tip Classification Model"
      ],
      "metadata": {
        "id": "Xq8poZdLNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the dataframe into training and testing sets\n",
        "train_df,test_df = GetTrainingAndTest(df)\n",
        "#Create a DictVectorizer object to convert feature dictionaries to feature arrays\n",
        "dvec = DictVectorizer()\n",
        "#Convert the feature dictionaries in the training set to feature arrays\n",
        "dict_df = train_df[feature_cols].T.to_dict().values()\n",
        "train_data = pd.DataFrame(dvec.fit_transform(dict_df).toarray())\n",
        "#Convert the feature dictionaries in the testing set to feature arrays using the same DictVectorizer object as the training set\n",
        "dict_df = test_df[feature_cols].T.to_dict().values()\n",
        "test_data = pd.DataFrame(dvec.transform(dict_df).toarray())\n",
        "\n",
        "X_train = train_data\n",
        "Y_train =  train_df['tip_class'].reshape(len(train_df['tip_class']), 1)\n",
        "\n",
        "X_dev = test_data\n",
        "Y_dev = test_df['tip_class'].reshape(len(test_df['tip_class']), 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JVkBVmXkNlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "p3ohZqA2NlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_SVM(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "e_H-Mp35NlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decision Tree Classifier"
      ],
      "metadata": {
        "id": "CxqlpRCyNlUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_decision_tree(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "4RXFigB9NlUM"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TIP PERCENT CLASSIFIFCATION MODEL"
      ],
      "metadata": {
        "id": "-l4sDyhhNlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_decision_tree(X_train, Y_train, Y_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lSD9ERzINlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the dataframe into training and testing sets\n",
        "train_df,test_df = GetTrainingAndTest(df)\n",
        "#Create a DictVectorizer object to convert feature dictionaries to feature arrays\n",
        "dvec = DictVectorizer()\n",
        "#Convert the feature dictionaries in the training set to feature arrays\n",
        "dict_df = train_df[feature_cols].T.to_dict().values()\n",
        "train_data = pd.DataFrame(dvec.fit_transform(dict_df).toarray())\n",
        "#Convert the feature dictionaries in the testing set to feature arrays using the same DictVectorizer object as the training set\n",
        "dict_df = test_df[feature_cols].T.to_dict().values()\n",
        "test_data = pd.DataFrame(dvec.transform(dict_df).toarray())\n",
        "\n",
        "X_train = train_data\n",
        "Y_train =  train_df['cluster_percent_class'].reshape(len(train_df['cluster_percent_class']), 1)\n",
        "\n",
        "X_dev = test_data\n",
        "Y_dev = test_df['cluster_percent_class'].reshape(len(test_df['cluster_percent_class']), 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YUb5ef1nNlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tip Amount Prediction Regression Model"
      ],
      "metadata": {
        "id": "tmpXKaT2NlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the dataset into training and test dataframes\n",
        "train_df,test_df = GetTrainingAndTest(df)\n",
        "#Convert the categorical features to numeric using DictVectorizer\n",
        "dvec = DictVectorizer()\n",
        "dict_df = train_df[feature_cols].T.to_dict().values()\n",
        "train_data = pd.DataFrame(dvec.fit_transform(dict_df).toarray())\n",
        "\n",
        "dict_df = test_df[feature_cols].T.to_dict().values()\n",
        "test_data = pd.DataFrame(dvec.transform(dict_df).toarray())\n",
        "\n",
        "X_train = train_data\n",
        "Y_train =  train_df[' tip_amount'].reshape(len(train_df[' tip_amount']), 1)\n",
        "\n",
        "X_dev = test_data\n",
        "Y_dev = test_df[' tip_amount'].reshape(len(test_df[' tip_amount']), 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WUmakouONlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Baseline Accuracy for Regression"
      ],
      "metadata": {
        "id": "nWE5iiHhNlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the Mean Absolute Error (MAE) of tip amount compared to the mean tip amount\n",
        "tip_mean = df[' tip_amount'].mean()\n",
        "\n",
        "diff_df = df[' tip_amount'].apply(lambda(x): abs(x-tip_mean))\n",
        "mae = diff_df.sum()/df.shape[0]\n",
        "print \"Mean absolute Error: %f\" %(mae)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NgtuShivNlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Linear Regression Model"
      ],
      "metadata": {
        "id": "vJKRTYXONlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting the tip amount as the target variable for regression\n",
        "Y_reg_train =  train_df[' tip_amount'].reshape(len(train_df[' tip_amount']), 1)\n",
        "Y_reg_dev = test_df[' tip_amount'].reshape(len(test_df[' tip_amount']), 1)\n",
        "#Initializing a linear regression model and fitting it on the training data\n",
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(X_train, Y_reg_train.reshape(Y_train.shape[0],))\n",
        "#Predicting tip amounts for the test set and evaluating the model's accuracy\n",
        "Y_pred_dev = lr.predict(X_dev)\n",
        "Y_pred_dev = Y_pred_dev.reshape(len(Y_pred_dev), 1)\n",
        "print \"Linear Regression Accuracy:%f\" %(lr.score(X_dev, Y_reg_dev))\n",
        "#Calculating the mean absolute error of the model\n",
        "mae = np.sum(np.absolute(Y_reg_dev - Y_pred_dev))/Y_reg_dev.shape[0]\n",
        "print \"Mean absolute Error: %f\" %(mae)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ON_fGWN9NlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####SVM Regression Model"
      ],
      "metadata": {
        "id": "LcKsDsJNNlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a support vector regression model to the training data\n",
        "\n",
        "svr = svm.SVR()\n",
        "svr.fit(X_train, Y_reg_train.reshape(Y_train.shape[0],))\n",
        "# Evaluate the model on the test data and print the R^2 score\n",
        "# This score represents the proportion of the variance in the target variable explained by the model\n",
        "print svr.score(X_dev, Y_reg_dev)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hmEm7JYoNlUN"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Lasso Regression Model"
      ],
      "metadata": {
        "id": "mRg8fC6dNlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = linear_model.Lasso()\n",
        "lasso.fit(X_train, Y_reg_train.reshape(Y_train.shape[0],))\n",
        "Y_pred_dev = lasso.predict(X_dev).reshape(len(Y_pred_dev), 1)\n",
        "print \"Lasso Regression Accuracy:%f\" %(lasso.score(X_dev, Y_reg_dev))\n",
        "\n",
        "mae = np.sum(np.absolute(Y_reg_dev - Y_pred_dev))/Y_reg_dev.shape[0]\n",
        "print \"Mean absolute Error: %f\" %(mae)\n",
        "\n",
        "# print lasso.coefs"
      ],
      "outputs": [],
      "metadata": {
        "id": "peedebzvNlUO"
      },
      "execution_count": null
    }
  ]
}